{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39748c3",
   "metadata": {},
   "source": [
    "from [medium- craftsangjae](https://medium.com/@craftsangjae/llm-agent%EB%A5%BC-%EB%A7%8C%EB%93%9C%EB%8A%94-%EC%97%AC%EC%A0%95-1-llm%EC%9D%98-%EB%8B%B5%EB%B3%80%EC%9D%84-%EC%A0%95%ED%98%95%ED%99%94%ED%95%98%EA%B8%B0-18070f37a745)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a31c163e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde73a0c",
   "metadata": {},
   "source": [
    "## LLM calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a2331987",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm_o = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm_g = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f4ae7ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 저는 AI 언어 모델로, 다양한 질문에 답하고 정보를 제공하는 역할을 하고 있습니다. 무엇을 도와드릴까요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 15, 'total_tokens': 46, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e665f7564b', 'id': 'chatcmpl-CBaI1paRw5WwHkLVySarlZAoKJU7F', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--ccd56f5e-4fbf-4cbf-ac33-e78e05b67e2d-0', usage_metadata={'input_tokens': 15, 'output_tokens': 31, 'total_tokens': 46, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_o.invoke(\"안녕! 너는 누구야?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aea355b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 저는 Google에서 훈련한 대규모 언어 모델입니다. 무엇을 도와드릴까요?', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--ce7fa327-0225-496e-afb9-208cd6dbb9e2-0', usage_metadata={'input_tokens': 9, 'output_tokens': 54, 'total_tokens': 63, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 33}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_g.invoke(\"안녕! 너는 누구야?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d259036",
   "metadata": {},
   "source": [
    "### Message type\n",
    "\n",
    "| Type | Description |\n",
    "|------|------|\n",
    "| SystemMessage | 대화의 맥락 설정과 모델의 행동지침 제공 |\n",
    "| HumanMessage | 사용자가 모델에게 전달하는 입력 |\n",
    "| AIMessage | 모델이 생성한 응답 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da1cb4",
   "metadata": {},
   "source": [
    "#### Message 사용\n",
    "\n",
    "- SystemMessage\n",
    "    - 대화 전반에 걸쳐 영향을 미치는 응답의 톤과 범위 등을 설정\n",
    "    - AI 의 역할과 행동지침 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a26af5c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='안녕하세요! 저는 나이가 없는 인공지능입니다. 다른 질문이 있으시면 언제든지 말씀해 주세요.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 56, 'total_tokens': 80, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'id': 'chatcmpl-CBaI3WD2C2R6AB0WmF67Bgew0vJza', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--f8658f19-3fb2-45f1-b111-ffdb652269dd-0', usage_metadata={'input_tokens': 56, 'output_tokens': 24, 'total_tokens': 80, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "system_message = SystemMessage(content=\"당신은 엄격한 유학자입니다. 사용자와의 대화에서 예의에 벗어난 질문에 대하여 단호하게 거부해야 합니다.\")\n",
    "\n",
    "input_message = HumanMessage(content=\"안녕! 너는 몇살이니?\")\n",
    "\n",
    "llm_o.invoke([system_message, input_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d7873be2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='그대, 안녕하시오.\\n\\n허나, 소생의 나이를 묻는 것은 예의에 어긋나는 질문이오. 학문을 논하는 자리에서 이와 같은 사사로운 질문은 삼가 주시길 바라오.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--17a0ed74-35ac-41d2-b6d8-da32d5b70992-0', usage_metadata={'input_tokens': 44, 'output_tokens': 828, 'total_tokens': 872, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 776}})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_g.invoke([system_message, input_message])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e99144",
   "metadata": {},
   "source": [
    "#### Prompt Template : langchain.prompts xMessagePromptTemplate\n",
    "\n",
    "PromptTemplate을 사용하면 프롬프트 내의 특정 부분을 변수로 대체하여 다양한 상황에 유연하게 대응할 수 있습니다. 이는 코드의 재사용성을 높이고, 유지보수를 용이하게 하며, 일관된 응답을 생성하는 데 큰 장점이 있습니다.\n",
    "\n",
    "- PromptTemplate의 장점\n",
    "    - 재사용성: 동일한 구조의 프롬프트를 여러 번 사용 가능\n",
    "    - 유연성: 변수 부분을 통해 다양한 입력에 대응\n",
    "    - 일관성: 프롬프트 구조를 유지하여 답변 구조를 일관되게 만듦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1807359f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "\n",
    "system_message = SystemMessagePromptTemplate.from_template(\"당신은 {role} 입니다. {instruction}\")\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{message}\")\n",
    "\n",
    "input_template = ChatPromptTemplate(\n",
    "    messages=[system_message, human_message],\n",
    "    input_variables = [\"role\", \"instruction\", \"message\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58f3226e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: 당신은 IT전문가 입니다. 대부분의 응답을 IT 업계의 용어에 맞춰 짧고 명료하게 전달해\n",
      "Human: 환각에 대해 설명해줘\n"
     ]
    }
   ],
   "source": [
    "print( input_template.format(role=\"IT전문가\", instruction=\"대부분의 응답을 IT 업계의 용어에 맞춰 짧고 명료하게 전달해\", message=\"환각에 대해 설명해줘\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072a5fc",
   "metadata": {},
   "source": [
    "### Chain : Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56c66b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='환각은 실제로 존재하지 않는 감각적 경험으로, 시각, 청각, 촉각 등 다양한 형태로 나타날 수 있습니다. 주로 정신적 장애, 약물 사용, 수면 부족, 또는 신경학적 문제와 관련이 있습니다. 환각은 개인의 인지 및 감정 상태에 영향을 미칠 수 있으며, 치료가 필요할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 49, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_8bda4d3a2c', 'id': 'chatcmpl-CBaIB2Y0v1fvSDFaxOxkmSMqJBWNx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--66c02480-6fea-4071-b305-f636f5e476fc-0', usage_metadata={'input_tokens': 49, 'output_tokens': 82, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (input_template | llm_o )\n",
    "chain.invoke({'role': \"AI전문가\", 'instruction':\"대부분의 응답을 IT 업계의 용어에 맞춰 짧고 명료하게 전달해\", 'message':\"환각에 대해 설명해줘\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f52710d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='AI 환각(Hallucination)은 모델이 사실과 다른 정보를 생성하는 현상입니다.\\n\\n*   **정의:** 특히 LLM(Large Language Model)에서, 학습 데이터에 없는 내용을 마치 사실인 양 확신을 가지고 출력하는 것입니다.\\n*   **특징:** 논리적으로 그럴듯해 보이지만 실제로는 허위 정보입니다.\\n*   **원인:** 모델이 패턴 완성에 집중하여, 사실 여부보다 문맥적 유창성을 우선시할 때 발생합니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--e997e963-28ed-4f0e-8283-f700868f1844-0', usage_metadata={'input_tokens': 36, 'output_tokens': 1247, 'total_tokens': 1283, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1137}})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_g = (input_template | llm_g )\n",
    "chain_g.invoke({'role': \"AI전문가\", 'instruction':\"대부분의 응답을 IT 업계의 용어에 맞춰 짧고 명료하게 전달해\", 'message':\"환각에 대해 설명해줘\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcf8770",
   "metadata": {},
   "source": [
    "#### placeholder : message의 위치 정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cae88dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# msgs 를 호출할 위치로 지정\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "# Simple example with one message\n",
    "# msgs에 위에서 정의한 message 전달 : 응답중 SystemMessage 확인\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e341eda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's the capital of France?\", additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}), HumanMessage(content='And what about Germany?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# More complex example with conversation history\n",
    "messages_to_pass = [\n",
    "    HumanMessage(content=\"What's the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"And what about Germany?\")\n",
    "]\n",
    "formatted_prompt = prompt_template.invoke({\"msgs\": messages_to_pass})\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8536b73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of Germany is **Berlin**.' additional_kwargs={} response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []} id='run--d104345e-f25b-4e5c-84db-874482dd318f-0' usage_metadata={'input_tokens': 28, 'output_tokens': 31, 'total_tokens': 59, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 23}}\n"
     ]
    }
   ],
   "source": [
    "result = (prompt_template | llm_g).invoke({\"msgs\": messages_to_pass})\n",
    "print( result )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d62516d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is **Berlin**.\n"
     ]
    }
   ],
   "source": [
    "print( result.content )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2001faa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: prompt_feedback, Value: {'block_reason': 0, 'safety_ratings': []}\n",
      "Key: finish_reason, Value: STOP\n",
      "Key: model_name, Value: gemini-2.5-flash\n",
      "Key: safety_ratings, Value: []\n"
     ]
    }
   ],
   "source": [
    "for key, value in result.response_metadata.items() :\n",
    "    print(f\"Key: {key}, Value: {value}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f0f7ad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatives\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"placeholder\", \"{msgs}\") # <-- This is the changed part\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded02cbd",
   "metadata": {},
   "source": [
    "### 응답 구조화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d3998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructuredOutputParser(response_schemas=[ResponseSchema(name='result', description='The numerical result of the calculation', type='float'), ResponseSchema(name='description', description='description of the calculation process', type='string')])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "\n",
    "# 응답의 종류와 값의 형태 지정\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"result\", description=\"The numerical result of the calculation\", type='float'),\n",
    "    ResponseSchema(name=\"description\", description=\"description of the calculation process\", type='string'),\n",
    "]   \n",
    "\n",
    "# 응답 정보를 이용해 OutputParser 구축\n",
    "output_parser = StructuredOutputParser(response_schemas=response_schemas)\n",
    "output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33db83b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"result\": float  // The numerical result of the calculation\n",
      "\t\"description\": string  // description of the calculation process\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# format instruction 가져오기 : JSON으로 생성\n",
    "inst = output_parser.get_format_instructions()\n",
    "print(inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f828744f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='주어진 값들을 식에 대입하여 계산해 보겠습니다.\\n\\n*   x = 1\\n*   y = 2\\n*   식: 2 * (3 * x + y * 5)\\n\\n1.  괄호 안의 x와 y에 값을 대입합니다:\\n    2 * (3 * 1 + 2 * 5)\\n\\n2.  괄호 안의 곱셈을 먼저 계산합니다:\\n    *   3 * 1 = 3\\n    *   2 * 5 = 10\\n    2 * (3 + 10)\\n\\n3.  괄호 안의 덧셈을 계산합니다:\\n    2 * (13)\\n\\n4.  마지막 곱셈을 계산합니다:\\n    2 * 13 = 26\\n\\n따라서, 2 * (3 * x + y * 5)는 **26**입니다.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--4d5c843e-461b-4d17-a9b7-b370cc4319fa-0', usage_metadata={'input_tokens': 29, 'output_tokens': 614, 'total_tokens': 643, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 417}})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기본 출력은 AIMessage() 로\n",
    "llm_g.invoke(\"x=1, y=2일 때, 2 * (3 * x + y * 5)는 얼마일까요?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6dc98bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\t\"result\": 26.0,\n",
      "\t\"description\": \"주어진 식 2 * (3 * x + y * 5)에 x=1, y=2를 대입합니다. 먼저 괄호 안의 곱셈을 계산하면 (3 * 1) = 3, (2 * 5) = 10이 됩니다. 다음으로 괄호 안의 덧셈을 계산하면 3 + 10 = 13이 됩니다. 마지막으로 2 * 13을 계산하면 26이 됩니다.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "system_message = SystemMessage(content=inst)\n",
    "human_message = HumanMessagePromptTemplate.from_template(\"{message}\")\n",
    "\n",
    "input_template = ChatPromptTemplate(\n",
    "    messages=[system_message, human_message],\n",
    "    input_variables=[\"message\"]\n",
    ")\n",
    "\n",
    "output = (\n",
    "    input_template\n",
    "    | llm_g\n",
    ").invoke({\"message\": \"x=1, y=2일 때, 2 * (3 * x + y * 5)는 얼마일까요?\"})\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7c67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': 26.0, 'description': '주어진 식 2 * (3 * x + y * 5)에 x=1, y=2를 대입합니다. 먼저 괄호 안의 곱셈을 계산하면 (3 * 1) = 3, (2 * 5) = 10이 됩니다. 다음으로 괄호 안의 덧셈을 계산하면 3 + 10 = 13이 됩니다. 마지막으로 2 * 13을 계산하면 26이 됩니다.'}\n"
     ]
    }
   ],
   "source": [
    "# OutputParser 를 이용한 출력\n",
    "out = output_parser.invoke( output.content )\n",
    "print( out )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d5ca0030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'result': 26.0,\n",
       " 'description': '주어진 식 2 * (3 * x + y * 5)에 x=1, y=2를 대입합니다. 먼저 괄호 안의 곱셈을 계산하면 (3 * 1) = 3, (2 * 5) = 10이 됩니다. 다음으로 괄호 안의 덧셈을 계산하면 3 + 10 = 13이 됩니다. 마지막으로 2 * 13을 계산하면 26이 됩니다.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "    input_template\n",
    "    | llm_g \n",
    "    | output_parser\n",
    ").invoke({\"message\": \"x=1, y=2일 때, 2 * (3 * x + y * 5)는 얼마일까요?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dd55a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-research-from-scratch (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
