{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd13c16",
   "metadata": {},
   "source": [
    "## LangChain Tool calling\n",
    "\n",
    "- Tool calling 은 Chat model 이 \"도구 호출\" 을 통해 주어진 프롬프트에 응답을 가능하게 함\n",
    "- 이름만 보면 무언가 실행할 것 같지만, 그렇지 않고 tool 에 인수(arguments)를 생성해서 전달\n",
    "- 일반적 기술로 model로 부터 구조화된 output을 생성하고 의도하지 않았더라도 이것을 사용할 수 있음\n",
    "\n",
    "실사용 사례로 비정형 텍스트로 부터 구조화된 output을 추출하는 것 \n",
    "\n",
    "![Tool Calling](https://python.langchain.com/v0.2/assets/images/tool_call-8d4a8b18e90cacd03f62e94071eceace.png)\n",
    "\n",
    "\n",
    "### tool schema 정의\n",
    "\n",
    "- model이 tool calling 을 가능하게 하기 위해 tool 이 수행하는 기능을 기술과 인수들을 정의한 tool schemas 를 전달해야 함\n",
    "- ChatModel의 경우 bind_tools() 메소드를 이용해 model 에 tool schema 전달\n",
    "- Tool Schema 는 파이썬 함수, Pydantic Models TypedDict class 또는 LangChain Tool objects 로 전달할 수 있으며, 이후 Model을 호출하면 프롬프트와 함께 이러한 Tool schema 가 전달\n",
    "\n",
    "#### Python 함수로 정의한 tool schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cfeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function name, type hints, and docstring are all part of the tool schema that's passed to the model. \n",
    "# Defining good, descriptive schemas is an extension of prompt engineering and is an important part of\n",
    "# getting models to perform well.\n",
    "# docstring 정의 중요 : 기능, 인수 설명\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two integers.\n",
    "\n",
    "    Args:\n",
    "        a: First integer\n",
    "        b: Second integer\n",
    "    \"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0428207d",
   "metadata": {},
   "source": [
    "#### LangChain Tool\n",
    "\n",
    "@tool 데코레이터 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6be4fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5104e55a",
   "metadata": {},
   "source": [
    "#### Pydantic class\n",
    "\n",
    "Pydantic 을 이용해 함께 제공하는(accompanying) 함수 없이도 schema 를 동등하게 정의할 수 있음\n",
    "\n",
    "- 열들은 기본 값을 정의하지 않으면 모든 필수입력(required)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d876230",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoonani/Development/follow/deep_research_from_scratch/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3699: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# BaseModel : Pydantic으로 정의하는 기본 모델, Field : 열 정의할 때 사용 (열 자료형 정의와 description 정의)\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class add(BaseModel):\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d90023d",
   "metadata": {},
   "source": [
    "#### TypedDict class\n",
    "\n",
    "> langchain-core>=0.2.25 \n",
    "> TypedDics 와 annotaions 를 사용할 수 있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0ebd3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated, TypedDict\n",
    "\n",
    "\n",
    "class add(TypedDict):\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "\n",
    "    # Annotations must have the type and can optionally include a default value and description (in that order).\n",
    "    a: Annotated[int, ..., \"First integer\"]\n",
    "    b: Annotated[int, ..., \"Second integer\"]\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "\n",
    "    a: Annotated[int, ..., \"First integer\"]\n",
    "    b: Annotated[int, ..., \"Second integer\"]\n",
    "\n",
    "\n",
    "tools = [add, multiply]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52e3f4b",
   "metadata": {},
   "source": [
    "### LLM with bind_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a105bb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key=os.environ[\"GOOGLE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2c7b031",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'function_call': {'name': 'multiply', 'arguments': '{\"a\": 3.0, \"b\": 12.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--8cf47b69-6ed0-4de0-aeed-b85ac79d4a46-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '98455d87-f586-4328-8e59-432e5a2762ba', 'type': 'tool_call'}], usage_metadata={'input_tokens': 102, 'output_tokens': 96, 'total_tokens': 198, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 77}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "query = \"What is 3 * 12?\"\n",
    "\n",
    "llm_with_tools.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16b9821",
   "metadata": {},
   "source": [
    "#### Tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8b91279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': 3.0, 'b': 12.0},\n",
       "  'id': 'a89c8c91-ff12-4fc8-b0e5-2454f9ac1101',\n",
       "  'type': 'tool_call'},\n",
       " {'name': 'add',\n",
       "  'args': {'a': 11.0, 'b': 49.0},\n",
       "  'id': 'af1e4aa3-f2db-4312-9fde-52d8407e56d6',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools.invoke(query).tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a2ffa3",
   "metadata": {},
   "source": [
    "#### parsing\n",
    "\n",
    "- 원하는 경우 출력 파서를 통해 출력을 추가로 처리할 수 있음\n",
    "- 예를 들어, .tool_calls에 채워진 기존 값을 PydanticToolsParser를 사용하여 Pydantic 객체로 변환할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d19e3827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[multiply(a=3, b=12)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import PydanticToolsParser\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "\n",
    "class add(BaseModel):\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "\n",
    "class multiply(BaseModel):\n",
    "    \"\"\"Multiply two integers.\"\"\"\n",
    "\n",
    "    a: int = Field(..., description=\"First integer\")\n",
    "    b: int = Field(..., description=\"Second integer\")\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "llm_with_tools = llm.bind_tools( [add, multiply] )\n",
    "\n",
    "chain = llm_with_tools | PydanticToolsParser(tools=[add, multiply])\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed12d518",
   "metadata": {},
   "source": [
    "## tool output 을 chat model에 전달하는 방법\n",
    "\n",
    "![](https://python.langchain.com/v0.2/assets/images/tool_invocation-7f277888701ee431a17607f1a035c080.png)\n",
    "\n",
    "![](https://python.langchain.com/v0.2/assets/images/tool_results-71b4b90f33a56563c102d91e7821a993.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f34cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "# llm 정의\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", api_key = os.environ[\"GOOGLE_API_KEY\"] )\n",
    "\n",
    "\n",
    "# tool 정의 : decorator 사용\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiplies a and b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# tool list\n",
    "tools = [add, multiply]\n",
    "\n",
    "# llm 에 tool 바인딩\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01376be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '071f596e-8ab7-4b6c-8687-4d9e4908cf0b', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11.0, 'b': 49.0}, 'id': 'f9d94ad1-4946-4435-b399-8c5bf3668b09', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"What is 3 * 12? Also, what is 11 + 49?\"\n",
    "\n",
    "messages = [HumanMessage(query)]\n",
    "\n",
    "# tool 을 바인딩한 model 실행 : HumanMessage 로 전달\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "\n",
    "#################################\n",
    "# tool_calls : tool calling 실행 #\n",
    "#################################\n",
    "print(ai_msg.tool_calls)\n",
    "\n",
    "# tool calling 으로 생성한 정보를 message 에 추가\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01c7c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='What is 3 * 12? Also, what is 11 + 49?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'function_call': {'name': 'add', 'arguments': '{\"a\": 11.0, \"b\": 49.0}'}}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--ffab870d-f202-4880-8a3f-01c5268d69ee-0', tool_calls=[{'name': 'multiply', 'args': {'a': 3.0, 'b': 12.0}, 'id': '071f596e-8ab7-4b6c-8687-4d9e4908cf0b', 'type': 'tool_call'}, {'name': 'add', 'args': {'a': 11.0, 'b': 49.0}, 'id': 'f9d94ad1-4946-4435-b399-8c5bf3668b09', 'type': 'tool_call'}], usage_metadata={'input_tokens': 116, 'output_tokens': 110, 'total_tokens': 226, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 71}}),\n",
       " ToolMessage(content='36', name='multiply', tool_call_id='071f596e-8ab7-4b6c-8687-4d9e4908cf0b'),\n",
       " ToolMessage(content='60', name='add', tool_call_id='f9d94ad1-4946-4435-b399-8c5bf3668b09')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tool 의 응답을 message 로 추가하기\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    selected_tool = {\"add\": add, \"multiply\": multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c38b6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='3 * 12 = 36 and 11 + 49 = 60.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--c5fd2fc5-219a-4999-ae14-3c10e03dbf48-0', usage_metadata={'input_tokens': 181, 'output_tokens': 22, 'total_tokens': 203, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tool 을 이용한 응답 생성\n",
    "llm_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800167c",
   "metadata": {},
   "source": [
    "Note that each ToolMessage must include a tool_call_id that matches an id in the original tool calls that the model generates. This helps the model match tool responses with tool calls.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-research-from-scratch (3.12.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
